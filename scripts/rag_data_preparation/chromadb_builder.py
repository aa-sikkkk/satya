#!/usr/bin/env python3
"""
ChromaDB Builder for Satya Learning System

Manages the creation and population of ChromaDB collections.
Implements the multi-collection architecture (Subject + Grade).

Features:
- Organized collection naming: `{dataset}_{subject}_grade_{grade}`
- NEB priority handling
- Batch ingestion
- Metadata validation
- Local persistence
"""

import os
import json
import logging
import chromadb
from typing import List, Dict, Any, Optional
from pathlib import Path
from tqdm import tqdm

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ChromaDBBuilder:
    """
    Manages Satya's Vector Database.
    """
    
    def __init__(self, db_path: str = "satya_data/chroma_db", reset: bool = False):
        """
        Initialize ChromaDB builder.
        
        Args:
            db_path: Path to persist database
            reset: If True, delete existing database and start fresh
        """
        self.db_path = db_path
        
        if reset and os.path.exists(db_path):
             logger.warning(f"‚ö†Ô∏è Resetting database at {db_path}...")
             import shutil
             shutil.rmtree(db_path)
             
        os.makedirs(db_path, exist_ok=True)
        
        try:
            self.client = chromadb.PersistentClient(path=db_path)
            logger.info(f"‚úÖ ChromaDB client initialized at {db_path}")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize ChromaDB: {e}")
            raise

    def get_collection_name(self, source: str, subject: str, grade: Any) -> str:
        """
        Generate standardized collection name.
        
        Format: {source}_{subject}_grade_{grade}
        Example: neb_science_grade_10, openstax_math_grade_9
        
        Args:
            source: Dataset source (neb, openstax, khanacademy, etc.)
            subject: Subject name (lowercase)
            grade: Grade level (8-12)
        """
        # Normalize inputs
        source = source.lower().replace(" ", "_")
        subject = subject.lower().replace(" ", "_")
        
        # Handle grade ranges if passed as string "8-10" etc, mostly it will be int
        grade_str = str(grade)
        
        name = f"{source}_{subject}_grade_{grade_str}"
        
        # Validate name rules for Chroma
        # Must be alphanumeric, underscores, hyphens, 3-63 chars
        safe_name = "".join(c for c in name if c.isalnum() or c in "_-")
        if len(safe_name) > 63:
            safe_name = safe_name[:63]
            
        return safe_name

    def create_or_get_collection(self, name: str, metadata: Dict = None):
        """Get existing collection or create new one."""
        try:
            return self.client.get_or_create_collection(
                name=name,
                metadata=metadata
            )
        except Exception as e:
            logger.error(f"‚ùå Error creating collection {name}: {e}")
            raise

    def ingest_chunks(
        self,
        chunks: List[Dict],
        collection_name: str,
        batch_size: int = 100
    ):
        """
        Ingest chunks into a specific collection.
        
        Args:
            chunks: List of chunk dictionaries containing 'text', 'embedding', 'metadata'
            collection_name: Target collection
            batch_size: Transaction size
        """
        if not chunks:
            logger.warning("Empty chunks list provided")
            return
            
        collection = self.create_or_get_collection(collection_name)
        
        logger.info(f"üì• Ingesting {len(chunks)} chunks into '{collection_name}'...")
        
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        with tqdm(total=len(chunks), desc=f"Writing to {collection_name}") as pbar:
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                
                # Prepare arrays for Chroma
                ids = []
                embeddings = []
                documents = []
                metadatas = []
                
                for idx, chunk in enumerate(batch):
                    # Generate unique ID
                    # Prefer existing ID, else generate one
                    chunk_id = chunk.get('chunk_id') or chunk.get('id') or f"{collection_name}_{i+idx}"
                    
                    # Validate embedding
                    embedding = chunk.get('embedding')
                    if not embedding:
                        logger.warning(f"Skipping chunk {chunk_id}: No embedding found")
                        continue
                        
                    ids.append(chunk_id)
                    embeddings.append(embedding)
                    documents.append(chunk.get('text', ''))
                    
                    # Clean metadata (Chroma requires str, int, float, bool)
                    base_metadata = chunk.get('metadata', {}).copy()
                    clean_metadata = {}
                    
                    # Flatten metadata and ensure types
                    for k, v in base_metadata.items():
                        if isinstance(v, (str, int, float, bool)):
                            clean_metadata[k] = v
                        elif v is None:
                            clean_metadata[k] = ""
                        else:
                            clean_metadata[k] = str(v)
                            
                    metadatas.append(clean_metadata)
                
                if ids:
                    try:
                        collection.upsert(
                            ids=ids,
                            embeddings=embeddings,
                            documents=documents,
                            metadatas=metadatas
                        )
                    except Exception as e:
                        logger.error(f"‚ùå Batch upsert failed: {e}")
                        
                pbar.update(len(batch))
                
        logger.info(f"‚úÖ Ingestion complete for {collection_name}")
        count = collection.count()
        logger.info(f"üìä Total items in collection: {count}")

    def list_collections(self):
        """List all available collections."""
        colls = self.client.list_collections()
        for c in colls:
            print(f"- {c.name} ({c.count()} items)")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Build ChromaDB collections from processed chunks")
    parser.add_argument("input_file", help="JSON file with chunks and embeddings")
    parser.add_argument("--collection", help="Collection name override")
    parser.add_argument("--source", help="Dataset source (e.g. neb, openstax)")
    parser.add_argument("--subject", help="Subject")
    parser.add_argument("--grade", help="Grade")
    
    args = parser.parse_args()
    
    builder = ChromaDBBuilder()
    
    # Check if we have explicit naming or need to infer
    if args.collection:
        collection_name = args.collection
    elif args.source and args.subject and args.grade:
        collection_name = builder.get_collection_name(args.source, args.subject, args.grade)
    else:
        # Try to infer from filename if simple
        base = os.path.basename(args.input_file)
        # simplistic fallback
        collection_name = base.replace(".json", "").replace("_chunks", "")
        
    if os.path.isdir(args.input_file):
        # Batch folder mode not fully implemented in CLI arg logic here for brevity, 
        # but class supports it.
        pass
    else:
        # Load file
        with open(args.input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        chunks = data if isinstance(data, list) else data.get('chunks', [])
        
        builder.ingest_chunks(chunks, collection_name)

if __name__ == "__main__":
    main()
